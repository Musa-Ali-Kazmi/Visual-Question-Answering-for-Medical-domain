{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8QKGLuiqRGAA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import Utility\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from typing import Callable, Dict, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQid1ntoSKCZ",
    "outputId": "1708cab2-5c11-4565-d040-3ce875991fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size= 4500\n",
      "val data size= 500\n"
     ]
    }
   ],
   "source": [
    "train_images, train_data = Utility.read_dataset(\"train\",\"2020\")\n",
    "test_images, test_data = Utility.read_dataset(\"val\",\"2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Q1X5zMlOVyt2",
    "outputId": "d70e87eb-1b44-45ab-f6ba-c42115d13a33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/vqa/DeepFusion/utils.py:47: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df = df.apply(lambda x: x.str.replace(\"(\",\" \"))\n",
      "/home/student/vqa/DeepFusion/utils.py:48: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df = df.apply(lambda x: x.str.replace(\")\",\" \"))\n",
      "/home/student/vqa/DeepFusion/utils.py:49: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df = df.apply(lambda x: x.str.replace('\\\\',\" \"))\n",
      "/home/student/vqa/DeepFusion/utils.py:52: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df = df.apply(lambda x: x.str.replace('?',''))\n"
     ]
    }
   ],
   "source": [
    "train_data['question_parsed'], train_data['answer_parsed'] = Utility.clean(train_data)\n",
    "test_data['question_parsed'], test_data['answer_parsed'] = Utility.clean(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "M6oWeWgjWwf2",
    "outputId": "96648f2e-af3b-448b-b270-e9fe826e392b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_parsed</th>\n",
       "      <th>answer_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/train_2020/train_images/synpic25788.jpg</td>\n",
       "      <td>is this a normal gastrointestinal image?</td>\n",
       "      <td>yes</td>\n",
       "      <td>is this a normal gastrointestinal image</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/train_2020/train_images/synpic36500.jpg</td>\n",
       "      <td>is the x-ray normal?</td>\n",
       "      <td>yes</td>\n",
       "      <td>is the x ray normal</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/train_2020/train_images/synpic24022.jpg</td>\n",
       "      <td>is this image normal?</td>\n",
       "      <td>yes</td>\n",
       "      <td>is this image normal</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data/train_2020/train_images/synpic25624.jpg</td>\n",
       "      <td>is there an abnormality in the x-ray?</td>\n",
       "      <td>no</td>\n",
       "      <td>is there an abnormality in the x ray</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data/train_2020/train_images/synpic35678.jpg</td>\n",
       "      <td>is there evidence of any abnormalities?</td>\n",
       "      <td>no</td>\n",
       "      <td>is there evidence of any abnormalities</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image_name  \\\n",
       "0  Data/train_2020/train_images/synpic25788.jpg   \n",
       "1  Data/train_2020/train_images/synpic36500.jpg   \n",
       "2  Data/train_2020/train_images/synpic24022.jpg   \n",
       "3  Data/train_2020/train_images/synpic25624.jpg   \n",
       "4  Data/train_2020/train_images/synpic35678.jpg   \n",
       "\n",
       "                                   question answer  \\\n",
       "0  is this a normal gastrointestinal image?    yes   \n",
       "1                      is the x-ray normal?    yes   \n",
       "2                     is this image normal?    yes   \n",
       "3     is there an abnormality in the x-ray?     no   \n",
       "4   is there evidence of any abnormalities?     no   \n",
       "\n",
       "                           question_parsed answer_parsed  \n",
       "0  is this a normal gastrointestinal image           yes  \n",
       "1                      is the x ray normal           yes  \n",
       "2                     is this image normal           yes  \n",
       "3     is there an abnormality in the x ray            no  \n",
       "4   is there evidence of any abnormalities            no  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['image_name'] = train_images\n",
    "test_data['image_name'] = test_images\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing train set into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(train_data, test_size=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332\n"
     ]
    }
   ],
   "source": [
    "class_names = sorted(set(train_data['answer_parsed']))\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = { i : class_names[i] for i in range(0, len(class_names) ) }\n",
    "class_dict = {y:x for x,y in class_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "class_names_val = sorted(set(val_data['answer_parsed']))\n",
    "print(len(class_names_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnHnA_nbaY6V",
    "outputId": "ba236ebd-8f8d-4d0d-9a56-2b5c12f0c0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "class_names_test = sorted(set(test_data['answer_parsed']))\n",
    "print(len(class_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = set(class_names)\n",
    "val = set(class_names_val)\n",
    "test = set(class_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all class names in val are present in train\n",
    "len(train.intersection(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all class names in test are present in train\n",
    "len(train.intersection(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(c in train for c in val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(c in train for c in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedVQA(torch.utils.data.Dataset):\n",
    "    def __init__(self, mode, data, class_dict, class_names_val,class_names_test,transform = None):\n",
    "        #these class_dict and class_names_val may differ, there may be unseen classes in class_names_val\n",
    "        def get_images(data, class_name):\n",
    "                temp_df = data[data['answer_parsed'] == class_name]\n",
    "                images = [x for x in temp_df['image_name']]\n",
    "                #print(f'Found {len(images)} {class_name} examples')\n",
    "                return images\n",
    "            \n",
    "        self.df = data\n",
    "        self.class_names = list(class_dict.keys())\n",
    "        self.class_names_val=class_names_val\n",
    "        self.class_names_test = class_names_test\n",
    "        self.mode= mode\n",
    "        self.class_dict=class_dict\n",
    "        #print(self.class_names)\n",
    "        self.images = {} #this is the dictionary where key is answer and value is a list of all images for that answer\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            \n",
    "              for class_name in self.class_names:\n",
    "                self.images[class_name] = get_images(data, class_name)\n",
    "        elif self.mode == 'val':\n",
    "              for class_name in self.class_names_val:\n",
    "                self.images[class_name] = get_images(data, class_name)\n",
    "        else:\n",
    "            for class_name in self.class_names_test:\n",
    "                self.images[class_name] = get_images(data, class_name)\n",
    "\n",
    "        self.transform = transform    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            class_name = random.choice(self.class_names)\n",
    "        elif self.mode == 'val':\n",
    "            class_name = random.choice(self.class_names_val)\n",
    "        else:\n",
    "            class_name = random.choice(self.class_names_test)\n",
    "\n",
    "        index = index % len(self.images[class_name])\n",
    "        image_path = self.images[class_name][index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        question = self.df[self.df['image_name']==image_path]['question_parsed'].values\n",
    "        answer = self.df[self.df['image_name']==image_path]['answer_parsed'].values\n",
    "        #label = self.class_names.index(class_name)\n",
    "        label = self.class_dict[class_name]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, question[0], answer[0], label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xgzC61hRwF-S"
   },
   "outputs": [],
   "source": [
    "class XRayTransform:\n",
    "    \"\"\"XRayTransform base class.\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"XRayTransform: {}\".format(self.__class__.__name__)\n",
    "\n",
    "class HistogramNormalize(XRayTransform):\n",
    "    \"\"\"\n",
    "    Apply histogram normalization.\n",
    "    Args:\n",
    "        number_bins: Number of bins to use in histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_bins: int = 256):\n",
    "        self.number_bins = number_bins\n",
    "\n",
    "    def __call__(self, sample: Dict) -> Dict:\n",
    "        image = sample.numpy()\n",
    "\n",
    "        # get image histogram, bins is the list of bin edges, density=True gives the value of pds, such that integral is 1\n",
    "        image_histogram, bins = np.histogram(\n",
    "            image.flatten(), self.number_bins, density=True\n",
    "        )\n",
    "        cdf = image_histogram.cumsum()  # cumulative distribution function\n",
    "        cdf = 255 * cdf / cdf[-1]  # normalize\n",
    "        #cdf[-1] here is the total sum\n",
    "        # use linear interpolation of cdf to find new pixel values\n",
    "        image_equalized = np.interp(image.flatten(), bins[:-1], cdf)\n",
    "        image_equalized.reshape(image.shape)\n",
    "\n",
    "        sample = torch.tensor(image_equalized.reshape(image.shape)).to(\n",
    "            sample\n",
    "        )\n",
    "\n",
    "        return sample\n",
    "\n",
    "class TensorToRGB(XRayTransform):\n",
    "    \"\"\"\n",
    "    Convert Tensor to RGB by replicating channels.\n",
    "    Args:\n",
    "        num_output_channels: Number of output channels (3 for RGB).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_output_channels: int = 3):\n",
    "        self.num_output_channels = num_output_channels\n",
    "\n",
    "    def __call__(self, sample: Dict) -> Dict:\n",
    "        expands = list()\n",
    "        for i in range(sample.ndim):\n",
    "            if i == 0:\n",
    "                expands.append(self.num_output_channels)\n",
    "            else:\n",
    "                expands.append(-1)\n",
    "        sample = sample.expand(*expands)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EotHPT0gYENT"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.Resize(size=(224,224)),\n",
    "                                 transforms.CenterCrop(size=(224,224)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.RandomVerticalFlip(),\n",
    "                                 transforms.RandomRotation(10),\n",
    "                                 transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean = np.array([0.2577, 0.2578, 0.2578]),\n",
    "                                  std=np.array([0.2206, 0.2206, 0.2205])),\n",
    "                                 #HistogramNormalize(),\n",
    "                                 #TensorToRGB()\n",
    "\n",
    "    ]),\n",
    "    'test': transforms.Compose([transforms.Resize(size=(224,224)),\n",
    "                                #transforms.CenterCrop(size=(224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = [0.2577, 0.2578, 0.2578],\n",
    "                                std=[0.2206, 0.2206, 0.2205]),\n",
    "#                                 HistogramNormalize(),\n",
    "#                                 TensorToRGB()    \n",
    "    ])\n",
    "\n",
    "}\n",
    "train_dataset = MedVQA('train', train_data, class_dict, class_names_val,class_names_test, data_transforms['train'])\n",
    "val_dataset = MedVQA('val', val_data, class_dict, class_names_val,class_names_test, data_transforms['test'])\n",
    "test_dataset = MedVQA('test', test_data, class_dict,  class_names_val,class_names_test,data_transforms['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dD7lY3ya1Ieh",
    "outputId": "d2a468b1-5265-46c3-bf3a-ac07a31eb2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches 125\n",
      "Number of training batches 16\n",
      "Number of test batches 16\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dl_val = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('Number of training batches', len(dl_train))\n",
    "print('Number of training batches', len(dl_val))\n",
    "print('Number of test batches', len(dl_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P6680yGCGYzG"
   },
   "outputs": [],
   "source": [
    "class_names = list(class_dict.keys())\n",
    "\n",
    "def show_images(images,questions, labels, preds):\n",
    "    plt.figure(figsize=(15,12))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(2, 3, i + 1, xticks=[], yticks=[])\n",
    "        image = (image.numpy()*255).transpose((1, 2, 0))\n",
    "        #mean = np.array([0.485, 0.456, 0.406])\n",
    "        #std = np.array([0.229, 0.224, 0.225])\n",
    "        #image = image * std + mean\n",
    "        image = np.clip(image, 0., 1.)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        col = 'green' if preds[i] == labels[i] else 'red'\n",
    "        \n",
    "        plt.title(f'{questions[i]}', fontsize = 10)\n",
    "        plt.xlabel(f'{labels[i]}', fontsize = 10)\n",
    "        plt.ylabel(f'{preds[i]}', color = col, fontsize = 10)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pretrained weights\n",
    "moco_model is the name of our model, you can substitute it with your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load('moco_model.ckpt')['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {}\n",
    "for k, v in pretrained_dict.items():\n",
    "    if k.startswith(\"model.encoder_q.\"):\n",
    "        k = k.replace(\"model.encoder_q.\", \"\")\n",
    "        state_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del state_dict['classifier.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del state_dict['classifier.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {\"classifier.weight\" if key == 'classifier.2.weight' else key:value for key, value in state_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {\"classifier.bias\" if key == 'classifier.2.bias' else key:value for key, value in state_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8zSQWqpoMnuu"
   },
   "outputs": [],
   "source": [
    "if \"model.encoder_q.classifier.2.weight\" in pretrained_dict.keys():\n",
    "    feature_dim = pretrained_dict[\n",
    "        \"model.encoder_q.classifier.2.weight\"\n",
    "    ].shape[0]\n",
    "    in_features = pretrained_dict[\n",
    "        \"model.encoder_q.classifier.2.weight\"\n",
    "    ].shape[1]\n",
    "\n",
    "    model = torchvision.models.__dict__['densenet121'](num_classes = feature_dim)\n",
    "    model.load_state_dict(state_dict)\n",
    "    del model.classifier\n",
    "    model.add_module(\n",
    "         \"classifier\", nn.Linear(in_features, 512)\n",
    "     )\n",
    "    model.classifier = nn.Sequential(model.classifier,\n",
    "                                     nn.BatchNorm1d(512),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Linear(512,332))\n",
    "else:\n",
    "    raise RuntimeError(\"Unrecognized classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=332, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing everything except final classifier layer\n",
    "c = 0\n",
    "for child in model.children():\n",
    "    if c < 1:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.weight : True\n",
      "features.norm0.weight : True\n",
      "features.norm0.bias : True\n",
      "features.denseblock1.denselayer1.norm1.weight : True\n",
      "features.denseblock1.denselayer1.norm1.bias : True\n",
      "features.denseblock1.denselayer1.conv1.weight : True\n",
      "features.denseblock1.denselayer1.norm2.weight : True\n",
      "features.denseblock1.denselayer1.norm2.bias : True\n",
      "features.denseblock1.denselayer1.conv2.weight : True\n",
      "features.denseblock1.denselayer2.norm1.weight : True\n",
      "features.denseblock1.denselayer2.norm1.bias : True\n",
      "features.denseblock1.denselayer2.conv1.weight : True\n",
      "features.denseblock1.denselayer2.norm2.weight : True\n",
      "features.denseblock1.denselayer2.norm2.bias : True\n",
      "features.denseblock1.denselayer2.conv2.weight : True\n",
      "features.denseblock1.denselayer3.norm1.weight : True\n",
      "features.denseblock1.denselayer3.norm1.bias : True\n",
      "features.denseblock1.denselayer3.conv1.weight : True\n",
      "features.denseblock1.denselayer3.norm2.weight : True\n",
      "features.denseblock1.denselayer3.norm2.bias : True\n",
      "features.denseblock1.denselayer3.conv2.weight : True\n",
      "features.denseblock1.denselayer4.norm1.weight : True\n",
      "features.denseblock1.denselayer4.norm1.bias : True\n",
      "features.denseblock1.denselayer4.conv1.weight : True\n",
      "features.denseblock1.denselayer4.norm2.weight : True\n",
      "features.denseblock1.denselayer4.norm2.bias : True\n",
      "features.denseblock1.denselayer4.conv2.weight : True\n",
      "features.denseblock1.denselayer5.norm1.weight : True\n",
      "features.denseblock1.denselayer5.norm1.bias : True\n",
      "features.denseblock1.denselayer5.conv1.weight : True\n",
      "features.denseblock1.denselayer5.norm2.weight : True\n",
      "features.denseblock1.denselayer5.norm2.bias : True\n",
      "features.denseblock1.denselayer5.conv2.weight : True\n",
      "features.denseblock1.denselayer6.norm1.weight : True\n",
      "features.denseblock1.denselayer6.norm1.bias : True\n",
      "features.denseblock1.denselayer6.conv1.weight : True\n",
      "features.denseblock1.denselayer6.norm2.weight : True\n",
      "features.denseblock1.denselayer6.norm2.bias : True\n",
      "features.denseblock1.denselayer6.conv2.weight : True\n",
      "features.transition1.norm.weight : True\n",
      "features.transition1.norm.bias : True\n",
      "features.transition1.conv.weight : True\n",
      "features.denseblock2.denselayer1.norm1.weight : True\n",
      "features.denseblock2.denselayer1.norm1.bias : True\n",
      "features.denseblock2.denselayer1.conv1.weight : True\n",
      "features.denseblock2.denselayer1.norm2.weight : True\n",
      "features.denseblock2.denselayer1.norm2.bias : True\n",
      "features.denseblock2.denselayer1.conv2.weight : True\n",
      "features.denseblock2.denselayer2.norm1.weight : True\n",
      "features.denseblock2.denselayer2.norm1.bias : True\n",
      "features.denseblock2.denselayer2.conv1.weight : True\n",
      "features.denseblock2.denselayer2.norm2.weight : True\n",
      "features.denseblock2.denselayer2.norm2.bias : True\n",
      "features.denseblock2.denselayer2.conv2.weight : True\n",
      "features.denseblock2.denselayer3.norm1.weight : True\n",
      "features.denseblock2.denselayer3.norm1.bias : True\n",
      "features.denseblock2.denselayer3.conv1.weight : True\n",
      "features.denseblock2.denselayer3.norm2.weight : True\n",
      "features.denseblock2.denselayer3.norm2.bias : True\n",
      "features.denseblock2.denselayer3.conv2.weight : True\n",
      "features.denseblock2.denselayer4.norm1.weight : True\n",
      "features.denseblock2.denselayer4.norm1.bias : True\n",
      "features.denseblock2.denselayer4.conv1.weight : True\n",
      "features.denseblock2.denselayer4.norm2.weight : True\n",
      "features.denseblock2.denselayer4.norm2.bias : True\n",
      "features.denseblock2.denselayer4.conv2.weight : True\n",
      "features.denseblock2.denselayer5.norm1.weight : True\n",
      "features.denseblock2.denselayer5.norm1.bias : True\n",
      "features.denseblock2.denselayer5.conv1.weight : True\n",
      "features.denseblock2.denselayer5.norm2.weight : True\n",
      "features.denseblock2.denselayer5.norm2.bias : True\n",
      "features.denseblock2.denselayer5.conv2.weight : True\n",
      "features.denseblock2.denselayer6.norm1.weight : True\n",
      "features.denseblock2.denselayer6.norm1.bias : True\n",
      "features.denseblock2.denselayer6.conv1.weight : True\n",
      "features.denseblock2.denselayer6.norm2.weight : True\n",
      "features.denseblock2.denselayer6.norm2.bias : True\n",
      "features.denseblock2.denselayer6.conv2.weight : True\n",
      "features.denseblock2.denselayer7.norm1.weight : True\n",
      "features.denseblock2.denselayer7.norm1.bias : True\n",
      "features.denseblock2.denselayer7.conv1.weight : True\n",
      "features.denseblock2.denselayer7.norm2.weight : True\n",
      "features.denseblock2.denselayer7.norm2.bias : True\n",
      "features.denseblock2.denselayer7.conv2.weight : True\n",
      "features.denseblock2.denselayer8.norm1.weight : True\n",
      "features.denseblock2.denselayer8.norm1.bias : True\n",
      "features.denseblock2.denselayer8.conv1.weight : True\n",
      "features.denseblock2.denselayer8.norm2.weight : True\n",
      "features.denseblock2.denselayer8.norm2.bias : True\n",
      "features.denseblock2.denselayer8.conv2.weight : True\n",
      "features.denseblock2.denselayer9.norm1.weight : True\n",
      "features.denseblock2.denselayer9.norm1.bias : True\n",
      "features.denseblock2.denselayer9.conv1.weight : True\n",
      "features.denseblock2.denselayer9.norm2.weight : True\n",
      "features.denseblock2.denselayer9.norm2.bias : True\n",
      "features.denseblock2.denselayer9.conv2.weight : True\n",
      "features.denseblock2.denselayer10.norm1.weight : True\n",
      "features.denseblock2.denselayer10.norm1.bias : True\n",
      "features.denseblock2.denselayer10.conv1.weight : True\n",
      "features.denseblock2.denselayer10.norm2.weight : True\n",
      "features.denseblock2.denselayer10.norm2.bias : True\n",
      "features.denseblock2.denselayer10.conv2.weight : True\n",
      "features.denseblock2.denselayer11.norm1.weight : True\n",
      "features.denseblock2.denselayer11.norm1.bias : True\n",
      "features.denseblock2.denselayer11.conv1.weight : True\n",
      "features.denseblock2.denselayer11.norm2.weight : True\n",
      "features.denseblock2.denselayer11.norm2.bias : True\n",
      "features.denseblock2.denselayer11.conv2.weight : True\n",
      "features.denseblock2.denselayer12.norm1.weight : True\n",
      "features.denseblock2.denselayer12.norm1.bias : True\n",
      "features.denseblock2.denselayer12.conv1.weight : True\n",
      "features.denseblock2.denselayer12.norm2.weight : True\n",
      "features.denseblock2.denselayer12.norm2.bias : True\n",
      "features.denseblock2.denselayer12.conv2.weight : True\n",
      "features.transition2.norm.weight : True\n",
      "features.transition2.norm.bias : True\n",
      "features.transition2.conv.weight : True\n",
      "features.denseblock3.denselayer1.norm1.weight : True\n",
      "features.denseblock3.denselayer1.norm1.bias : True\n",
      "features.denseblock3.denselayer1.conv1.weight : True\n",
      "features.denseblock3.denselayer1.norm2.weight : True\n",
      "features.denseblock3.denselayer1.norm2.bias : True\n",
      "features.denseblock3.denselayer1.conv2.weight : True\n",
      "features.denseblock3.denselayer2.norm1.weight : True\n",
      "features.denseblock3.denselayer2.norm1.bias : True\n",
      "features.denseblock3.denselayer2.conv1.weight : True\n",
      "features.denseblock3.denselayer2.norm2.weight : True\n",
      "features.denseblock3.denselayer2.norm2.bias : True\n",
      "features.denseblock3.denselayer2.conv2.weight : True\n",
      "features.denseblock3.denselayer3.norm1.weight : True\n",
      "features.denseblock3.denselayer3.norm1.bias : True\n",
      "features.denseblock3.denselayer3.conv1.weight : True\n",
      "features.denseblock3.denselayer3.norm2.weight : True\n",
      "features.denseblock3.denselayer3.norm2.bias : True\n",
      "features.denseblock3.denselayer3.conv2.weight : True\n",
      "features.denseblock3.denselayer4.norm1.weight : True\n",
      "features.denseblock3.denselayer4.norm1.bias : True\n",
      "features.denseblock3.denselayer4.conv1.weight : True\n",
      "features.denseblock3.denselayer4.norm2.weight : True\n",
      "features.denseblock3.denselayer4.norm2.bias : True\n",
      "features.denseblock3.denselayer4.conv2.weight : True\n",
      "features.denseblock3.denselayer5.norm1.weight : True\n",
      "features.denseblock3.denselayer5.norm1.bias : True\n",
      "features.denseblock3.denselayer5.conv1.weight : True\n",
      "features.denseblock3.denselayer5.norm2.weight : True\n",
      "features.denseblock3.denselayer5.norm2.bias : True\n",
      "features.denseblock3.denselayer5.conv2.weight : True\n",
      "features.denseblock3.denselayer6.norm1.weight : True\n",
      "features.denseblock3.denselayer6.norm1.bias : True\n",
      "features.denseblock3.denselayer6.conv1.weight : True\n",
      "features.denseblock3.denselayer6.norm2.weight : True\n",
      "features.denseblock3.denselayer6.norm2.bias : True\n",
      "features.denseblock3.denselayer6.conv2.weight : True\n",
      "features.denseblock3.denselayer7.norm1.weight : True\n",
      "features.denseblock3.denselayer7.norm1.bias : True\n",
      "features.denseblock3.denselayer7.conv1.weight : True\n",
      "features.denseblock3.denselayer7.norm2.weight : True\n",
      "features.denseblock3.denselayer7.norm2.bias : True\n",
      "features.denseblock3.denselayer7.conv2.weight : True\n",
      "features.denseblock3.denselayer8.norm1.weight : True\n",
      "features.denseblock3.denselayer8.norm1.bias : True\n",
      "features.denseblock3.denselayer8.conv1.weight : True\n",
      "features.denseblock3.denselayer8.norm2.weight : True\n",
      "features.denseblock3.denselayer8.norm2.bias : True\n",
      "features.denseblock3.denselayer8.conv2.weight : True\n",
      "features.denseblock3.denselayer9.norm1.weight : True\n",
      "features.denseblock3.denselayer9.norm1.bias : True\n",
      "features.denseblock3.denselayer9.conv1.weight : True\n",
      "features.denseblock3.denselayer9.norm2.weight : True\n",
      "features.denseblock3.denselayer9.norm2.bias : True\n",
      "features.denseblock3.denselayer9.conv2.weight : True\n",
      "features.denseblock3.denselayer10.norm1.weight : True\n",
      "features.denseblock3.denselayer10.norm1.bias : True\n",
      "features.denseblock3.denselayer10.conv1.weight : True\n",
      "features.denseblock3.denselayer10.norm2.weight : True\n",
      "features.denseblock3.denselayer10.norm2.bias : True\n",
      "features.denseblock3.denselayer10.conv2.weight : True\n",
      "features.denseblock3.denselayer11.norm1.weight : True\n",
      "features.denseblock3.denselayer11.norm1.bias : True\n",
      "features.denseblock3.denselayer11.conv1.weight : True\n",
      "features.denseblock3.denselayer11.norm2.weight : True\n",
      "features.denseblock3.denselayer11.norm2.bias : True\n",
      "features.denseblock3.denselayer11.conv2.weight : True\n",
      "features.denseblock3.denselayer12.norm1.weight : True\n",
      "features.denseblock3.denselayer12.norm1.bias : True\n",
      "features.denseblock3.denselayer12.conv1.weight : True\n",
      "features.denseblock3.denselayer12.norm2.weight : True\n",
      "features.denseblock3.denselayer12.norm2.bias : True\n",
      "features.denseblock3.denselayer12.conv2.weight : True\n",
      "features.denseblock3.denselayer13.norm1.weight : True\n",
      "features.denseblock3.denselayer13.norm1.bias : True\n",
      "features.denseblock3.denselayer13.conv1.weight : True\n",
      "features.denseblock3.denselayer13.norm2.weight : True\n",
      "features.denseblock3.denselayer13.norm2.bias : True\n",
      "features.denseblock3.denselayer13.conv2.weight : True\n",
      "features.denseblock3.denselayer14.norm1.weight : True\n",
      "features.denseblock3.denselayer14.norm1.bias : True\n",
      "features.denseblock3.denselayer14.conv1.weight : True\n",
      "features.denseblock3.denselayer14.norm2.weight : True\n",
      "features.denseblock3.denselayer14.norm2.bias : True\n",
      "features.denseblock3.denselayer14.conv2.weight : True\n",
      "features.denseblock3.denselayer15.norm1.weight : True\n",
      "features.denseblock3.denselayer15.norm1.bias : True\n",
      "features.denseblock3.denselayer15.conv1.weight : True\n",
      "features.denseblock3.denselayer15.norm2.weight : True\n",
      "features.denseblock3.denselayer15.norm2.bias : True\n",
      "features.denseblock3.denselayer15.conv2.weight : True\n",
      "features.denseblock3.denselayer16.norm1.weight : True\n",
      "features.denseblock3.denselayer16.norm1.bias : True\n",
      "features.denseblock3.denselayer16.conv1.weight : True\n",
      "features.denseblock3.denselayer16.norm2.weight : True\n",
      "features.denseblock3.denselayer16.norm2.bias : True\n",
      "features.denseblock3.denselayer16.conv2.weight : True\n",
      "features.denseblock3.denselayer17.norm1.weight : True\n",
      "features.denseblock3.denselayer17.norm1.bias : True\n",
      "features.denseblock3.denselayer17.conv1.weight : True\n",
      "features.denseblock3.denselayer17.norm2.weight : True\n",
      "features.denseblock3.denselayer17.norm2.bias : True\n",
      "features.denseblock3.denselayer17.conv2.weight : True\n",
      "features.denseblock3.denselayer18.norm1.weight : True\n",
      "features.denseblock3.denselayer18.norm1.bias : True\n",
      "features.denseblock3.denselayer18.conv1.weight : True\n",
      "features.denseblock3.denselayer18.norm2.weight : True\n",
      "features.denseblock3.denselayer18.norm2.bias : True\n",
      "features.denseblock3.denselayer18.conv2.weight : True\n",
      "features.denseblock3.denselayer19.norm1.weight : True\n",
      "features.denseblock3.denselayer19.norm1.bias : True\n",
      "features.denseblock3.denselayer19.conv1.weight : True\n",
      "features.denseblock3.denselayer19.norm2.weight : True\n",
      "features.denseblock3.denselayer19.norm2.bias : True\n",
      "features.denseblock3.denselayer19.conv2.weight : True\n",
      "features.denseblock3.denselayer20.norm1.weight : True\n",
      "features.denseblock3.denselayer20.norm1.bias : True\n",
      "features.denseblock3.denselayer20.conv1.weight : True\n",
      "features.denseblock3.denselayer20.norm2.weight : True\n",
      "features.denseblock3.denselayer20.norm2.bias : True\n",
      "features.denseblock3.denselayer20.conv2.weight : True\n",
      "features.denseblock3.denselayer21.norm1.weight : True\n",
      "features.denseblock3.denselayer21.norm1.bias : True\n",
      "features.denseblock3.denselayer21.conv1.weight : True\n",
      "features.denseblock3.denselayer21.norm2.weight : True\n",
      "features.denseblock3.denselayer21.norm2.bias : True\n",
      "features.denseblock3.denselayer21.conv2.weight : True\n",
      "features.denseblock3.denselayer22.norm1.weight : True\n",
      "features.denseblock3.denselayer22.norm1.bias : True\n",
      "features.denseblock3.denselayer22.conv1.weight : True\n",
      "features.denseblock3.denselayer22.norm2.weight : True\n",
      "features.denseblock3.denselayer22.norm2.bias : True\n",
      "features.denseblock3.denselayer22.conv2.weight : True\n",
      "features.denseblock3.denselayer23.norm1.weight : True\n",
      "features.denseblock3.denselayer23.norm1.bias : True\n",
      "features.denseblock3.denselayer23.conv1.weight : True\n",
      "features.denseblock3.denselayer23.norm2.weight : True\n",
      "features.denseblock3.denselayer23.norm2.bias : True\n",
      "features.denseblock3.denselayer23.conv2.weight : True\n",
      "features.denseblock3.denselayer24.norm1.weight : True\n",
      "features.denseblock3.denselayer24.norm1.bias : True\n",
      "features.denseblock3.denselayer24.conv1.weight : True\n",
      "features.denseblock3.denselayer24.norm2.weight : True\n",
      "features.denseblock3.denselayer24.norm2.bias : True\n",
      "features.denseblock3.denselayer24.conv2.weight : True\n",
      "features.transition3.norm.weight : True\n",
      "features.transition3.norm.bias : True\n",
      "features.transition3.conv.weight : True\n",
      "features.denseblock4.denselayer1.norm1.weight : True\n",
      "features.denseblock4.denselayer1.norm1.bias : True\n",
      "features.denseblock4.denselayer1.conv1.weight : True\n",
      "features.denseblock4.denselayer1.norm2.weight : True\n",
      "features.denseblock4.denselayer1.norm2.bias : True\n",
      "features.denseblock4.denselayer1.conv2.weight : True\n",
      "features.denseblock4.denselayer2.norm1.weight : True\n",
      "features.denseblock4.denselayer2.norm1.bias : True\n",
      "features.denseblock4.denselayer2.conv1.weight : True\n",
      "features.denseblock4.denselayer2.norm2.weight : True\n",
      "features.denseblock4.denselayer2.norm2.bias : True\n",
      "features.denseblock4.denselayer2.conv2.weight : True\n",
      "features.denseblock4.denselayer3.norm1.weight : True\n",
      "features.denseblock4.denselayer3.norm1.bias : True\n",
      "features.denseblock4.denselayer3.conv1.weight : True\n",
      "features.denseblock4.denselayer3.norm2.weight : True\n",
      "features.denseblock4.denselayer3.norm2.bias : True\n",
      "features.denseblock4.denselayer3.conv2.weight : True\n",
      "features.denseblock4.denselayer4.norm1.weight : True\n",
      "features.denseblock4.denselayer4.norm1.bias : True\n",
      "features.denseblock4.denselayer4.conv1.weight : True\n",
      "features.denseblock4.denselayer4.norm2.weight : True\n",
      "features.denseblock4.denselayer4.norm2.bias : True\n",
      "features.denseblock4.denselayer4.conv2.weight : True\n",
      "features.denseblock4.denselayer5.norm1.weight : True\n",
      "features.denseblock4.denselayer5.norm1.bias : True\n",
      "features.denseblock4.denselayer5.conv1.weight : True\n",
      "features.denseblock4.denselayer5.norm2.weight : True\n",
      "features.denseblock4.denselayer5.norm2.bias : True\n",
      "features.denseblock4.denselayer5.conv2.weight : True\n",
      "features.denseblock4.denselayer6.norm1.weight : True\n",
      "features.denseblock4.denselayer6.norm1.bias : True\n",
      "features.denseblock4.denselayer6.conv1.weight : True\n",
      "features.denseblock4.denselayer6.norm2.weight : True\n",
      "features.denseblock4.denselayer6.norm2.bias : True\n",
      "features.denseblock4.denselayer6.conv2.weight : True\n",
      "features.denseblock4.denselayer7.norm1.weight : True\n",
      "features.denseblock4.denselayer7.norm1.bias : True\n",
      "features.denseblock4.denselayer7.conv1.weight : True\n",
      "features.denseblock4.denselayer7.norm2.weight : True\n",
      "features.denseblock4.denselayer7.norm2.bias : True\n",
      "features.denseblock4.denselayer7.conv2.weight : True\n",
      "features.denseblock4.denselayer8.norm1.weight : True\n",
      "features.denseblock4.denselayer8.norm1.bias : True\n",
      "features.denseblock4.denselayer8.conv1.weight : True\n",
      "features.denseblock4.denselayer8.norm2.weight : True\n",
      "features.denseblock4.denselayer8.norm2.bias : True\n",
      "features.denseblock4.denselayer8.conv2.weight : True\n",
      "features.denseblock4.denselayer9.norm1.weight : True\n",
      "features.denseblock4.denselayer9.norm1.bias : True\n",
      "features.denseblock4.denselayer9.conv1.weight : True\n",
      "features.denseblock4.denselayer9.norm2.weight : True\n",
      "features.denseblock4.denselayer9.norm2.bias : True\n",
      "features.denseblock4.denselayer9.conv2.weight : True\n",
      "features.denseblock4.denselayer10.norm1.weight : True\n",
      "features.denseblock4.denselayer10.norm1.bias : True\n",
      "features.denseblock4.denselayer10.conv1.weight : True\n",
      "features.denseblock4.denselayer10.norm2.weight : True\n",
      "features.denseblock4.denselayer10.norm2.bias : True\n",
      "features.denseblock4.denselayer10.conv2.weight : True\n",
      "features.denseblock4.denselayer11.norm1.weight : True\n",
      "features.denseblock4.denselayer11.norm1.bias : True\n",
      "features.denseblock4.denselayer11.conv1.weight : True\n",
      "features.denseblock4.denselayer11.norm2.weight : True\n",
      "features.denseblock4.denselayer11.norm2.bias : True\n",
      "features.denseblock4.denselayer11.conv2.weight : True\n",
      "features.denseblock4.denselayer12.norm1.weight : True\n",
      "features.denseblock4.denselayer12.norm1.bias : True\n",
      "features.denseblock4.denselayer12.conv1.weight : True\n",
      "features.denseblock4.denselayer12.norm2.weight : True\n",
      "features.denseblock4.denselayer12.norm2.bias : True\n",
      "features.denseblock4.denselayer12.conv2.weight : True\n",
      "features.denseblock4.denselayer13.norm1.weight : True\n",
      "features.denseblock4.denselayer13.norm1.bias : True\n",
      "features.denseblock4.denselayer13.conv1.weight : True\n",
      "features.denseblock4.denselayer13.norm2.weight : True\n",
      "features.denseblock4.denselayer13.norm2.bias : True\n",
      "features.denseblock4.denselayer13.conv2.weight : True\n",
      "features.denseblock4.denselayer14.norm1.weight : True\n",
      "features.denseblock4.denselayer14.norm1.bias : True\n",
      "features.denseblock4.denselayer14.conv1.weight : True\n",
      "features.denseblock4.denselayer14.norm2.weight : True\n",
      "features.denseblock4.denselayer14.norm2.bias : True\n",
      "features.denseblock4.denselayer14.conv2.weight : True\n",
      "features.denseblock4.denselayer15.norm1.weight : True\n",
      "features.denseblock4.denselayer15.norm1.bias : True\n",
      "features.denseblock4.denselayer15.conv1.weight : True\n",
      "features.denseblock4.denselayer15.norm2.weight : True\n",
      "features.denseblock4.denselayer15.norm2.bias : True\n",
      "features.denseblock4.denselayer15.conv2.weight : True\n",
      "features.denseblock4.denselayer16.norm1.weight : True\n",
      "features.denseblock4.denselayer16.norm1.bias : True\n",
      "features.denseblock4.denselayer16.conv1.weight : True\n",
      "features.denseblock4.denselayer16.norm2.weight : True\n",
      "features.denseblock4.denselayer16.norm2.bias : True\n",
      "features.denseblock4.denselayer16.conv2.weight : True\n",
      "features.norm5.weight : True\n",
      "features.norm5.bias : True\n",
      "classifier.0.weight : True\n",
      "classifier.0.bias : True\n",
      "classifier.1.weight : True\n",
      "classifier.1.bias : True\n",
      "classifier.3.weight : True\n",
      "classifier.3.bias : True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, ':', param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtTJSUaSsVuy",
    "outputId": "275b479d-8401-49a0-aa31-fcc0d9e7b41f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "_hgAezqi4eY5"
   },
   "outputs": [],
   "source": [
    "def show_preds():\n",
    "    model.eval()\n",
    "    images,questions,_ ,labels = next(iter(dl_test))\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "\n",
    "    \n",
    "    images = images.to('cpu')\n",
    "    labels = labels.to('cpu')\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    show_images(images[:6], questions, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "SkezC88C4haq"
   },
   "outputs": [],
   "source": [
    "best_model = model\n",
    "\n",
    "def train(epochs):\n",
    "    print('Starting training..')\n",
    "    best_accuracy = 0\n",
    "    accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    for e in range(0, epochs):\n",
    "        print('='*16)\n",
    "        print(f'Starting epoch {e + 1}/{epochs}')\n",
    "        print('='*16)\n",
    "\n",
    "        train_loss = 0.\n",
    "        val_loss = 0.\n",
    "        \n",
    "        accuracy_avg = []\n",
    "        val_loss_avg = []\n",
    "        model.train() # set model to training phase\n",
    "\n",
    "        for train_step, (images, questions,_, labels) in enumerate(dl_train):\n",
    "\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #2scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            if train_step % 16 == 0:\n",
    "                print('Evaluating at step', train_step)\n",
    "\n",
    "                accuracy = 0\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "\n",
    "                    model.eval() # set model to eval phase\n",
    "\n",
    "                    for val_step, (images, question, _,  labels) in enumerate(dl_val):\n",
    "\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        outputs = model(images)\n",
    "                        loss = loss_fn(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        \n",
    "                        accuracy += sum((preds == labels))\n",
    "\n",
    "                    val_loss /= (val_step + 1)\n",
    "                    accuracy = accuracy/len(val_dataset)\n",
    "                    accuracy_avg.append(round(accuracy.item(),4))\n",
    "                    val_loss_avg.append(round(val_loss,4))\n",
    "                    if (accuracy > best_accuracy):\n",
    "                        best_model=model\n",
    "                        best_accuracy = accuracy\n",
    "                    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                if accuracy >= 0.95:\n",
    "                    print('Performance condition satisfied, stopping..')\n",
    "                    return\n",
    "\n",
    "        train_loss /= (train_step + 1)\n",
    "        accuracy_list.append(np.mean(accuracy_avg))\n",
    "        train_loss_list.append(round(train_loss,4))\n",
    "        val_loss_list.append(np.mean(val_loss_avg))\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "    print('Training complete..')\n",
    "    return accuracy_list, val_loss_list, train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZRw9pt884lk6",
    "outputId": "b3242824-323d-407c-8782-3e19170e171e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "================\n",
      "Starting epoch 1/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 5.9964, Accuracy: 0.0040\n",
      "Evaluating at step 16\n",
      "Validation Loss: 7.5351, Accuracy: 0.0020\n",
      "Evaluating at step 32\n",
      "Validation Loss: 7.0550, Accuracy: 0.0120\n",
      "Evaluating at step 48\n",
      "Validation Loss: 6.5140, Accuracy: 0.0180\n",
      "Evaluating at step 64\n",
      "Validation Loss: 6.0393, Accuracy: 0.0320\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.5493, Accuracy: 0.0820\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.1012, Accuracy: 0.1440\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.2353, Accuracy: 0.0920\n",
      "Training Loss: 5.0981\n",
      "================\n",
      "Starting epoch 2/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.7445, Accuracy: 0.1420\n",
      "Evaluating at step 16\n",
      "Validation Loss: 4.9285, Accuracy: 0.1460\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.1843, Accuracy: 0.0860\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.1870, Accuracy: 0.1520\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.0939, Accuracy: 0.1840\n",
      "Evaluating at step 80\n",
      "Validation Loss: 4.8962, Accuracy: 0.1940\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.0169, Accuracy: 0.1900\n",
      "Evaluating at step 112\n",
      "Validation Loss: 4.9707, Accuracy: 0.1860\n",
      "Training Loss: 3.1443\n",
      "================\n",
      "Starting epoch 3/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.5817, Accuracy: 0.1940\n",
      "Evaluating at step 16\n",
      "Validation Loss: 4.5540, Accuracy: 0.2440\n",
      "Evaluating at step 32\n",
      "Validation Loss: 4.9054, Accuracy: 0.2340\n",
      "Evaluating at step 48\n",
      "Validation Loss: 4.9364, Accuracy: 0.2380\n",
      "Evaluating at step 64\n",
      "Validation Loss: 4.6208, Accuracy: 0.2160\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.1191, Accuracy: 0.2080\n",
      "Evaluating at step 96\n",
      "Validation Loss: 4.7300, Accuracy: 0.2080\n",
      "Evaluating at step 112\n",
      "Validation Loss: 4.7924, Accuracy: 0.2580\n",
      "Training Loss: 2.4973\n",
      "================\n",
      "Starting epoch 4/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.8141, Accuracy: 0.2080\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.1474, Accuracy: 0.2220\n",
      "Evaluating at step 32\n",
      "Validation Loss: 4.9911, Accuracy: 0.2420\n",
      "Evaluating at step 48\n",
      "Validation Loss: 4.9510, Accuracy: 0.2340\n",
      "Evaluating at step 64\n",
      "Validation Loss: 4.7872, Accuracy: 0.2380\n",
      "Evaluating at step 80\n",
      "Validation Loss: 4.4841, Accuracy: 0.2580\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.1562, Accuracy: 0.2240\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.0849, Accuracy: 0.2280\n",
      "Training Loss: 1.9384\n",
      "================\n",
      "Starting epoch 5/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.7070, Accuracy: 0.2700\n",
      "Evaluating at step 16\n",
      "Validation Loss: 4.9249, Accuracy: 0.2220\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.2302, Accuracy: 0.2000\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.5024, Accuracy: 0.2080\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.3434, Accuracy: 0.2580\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.3454, Accuracy: 0.2300\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.0292, Accuracy: 0.2580\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.2029, Accuracy: 0.2600\n",
      "Training Loss: 1.6807\n",
      "================\n",
      "Starting epoch 6/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.8078, Accuracy: 0.2640\n",
      "Evaluating at step 16\n",
      "Validation Loss: 4.7502, Accuracy: 0.2700\n",
      "Evaluating at step 32\n",
      "Validation Loss: 4.9698, Accuracy: 0.2460\n",
      "Evaluating at step 48\n",
      "Validation Loss: 4.8746, Accuracy: 0.2780\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.3569, Accuracy: 0.2460\n",
      "Evaluating at step 80\n",
      "Validation Loss: 4.8415, Accuracy: 0.3320\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.0705, Accuracy: 0.2720\n",
      "Evaluating at step 112\n",
      "Validation Loss: 4.9323, Accuracy: 0.2620\n",
      "Training Loss: 1.4236\n",
      "================\n",
      "Starting epoch 7/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.7641, Accuracy: 0.2540\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.4065, Accuracy: 0.2640\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.1612, Accuracy: 0.2460\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.0556, Accuracy: 0.3020\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.4129, Accuracy: 0.2640\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.3615, Accuracy: 0.2880\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.3424, Accuracy: 0.2700\n",
      "Evaluating at step 112\n",
      "Validation Loss: 4.8781, Accuracy: 0.2500\n",
      "Training Loss: 1.2545\n",
      "================\n",
      "Starting epoch 8/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.6428, Accuracy: 0.3320\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.4563, Accuracy: 0.2720\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.3471, Accuracy: 0.2700\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.5214, Accuracy: 0.2780\n",
      "Evaluating at step 64\n",
      "Validation Loss: 4.9509, Accuracy: 0.2980\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.2487, Accuracy: 0.2940\n",
      "Evaluating at step 96\n",
      "Validation Loss: 4.9594, Accuracy: 0.3120\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.2969, Accuracy: 0.3100\n",
      "Training Loss: 1.1124\n",
      "================\n",
      "Starting epoch 9/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.5343, Accuracy: 0.3440\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.0706, Accuracy: 0.3120\n",
      "Evaluating at step 32\n",
      "Validation Loss: 4.7576, Accuracy: 0.3320\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.4045, Accuracy: 0.2700\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.2661, Accuracy: 0.2760\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.5677, Accuracy: 0.2720\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.5009, Accuracy: 0.2980\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.3101, Accuracy: 0.3060\n",
      "Training Loss: 0.9848\n",
      "================\n",
      "Starting epoch 10/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 4.9063, Accuracy: 0.2940\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.2083, Accuracy: 0.3080\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.8222, Accuracy: 0.3160\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.5672, Accuracy: 0.3320\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.8377, Accuracy: 0.3100\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.2900, Accuracy: 0.3420\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.2329, Accuracy: 0.3700\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.4706, Accuracy: 0.3460\n",
      "Training Loss: 0.8865\n",
      "================\n",
      "Starting epoch 11/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 5.7676, Accuracy: 0.3180\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.3071, Accuracy: 0.3240\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.9045, Accuracy: 0.3040\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.6790, Accuracy: 0.2880\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.4800, Accuracy: 0.3460\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.0034, Accuracy: 0.3420\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.2758, Accuracy: 0.3260\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.6334, Accuracy: 0.3080\n",
      "Training Loss: 0.8618\n",
      "================\n",
      "Starting epoch 12/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 5.2944, Accuracy: 0.3280\n",
      "Evaluating at step 16\n",
      "Validation Loss: 6.1128, Accuracy: 0.3260\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.8285, Accuracy: 0.3220\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.4361, Accuracy: 0.2880\n",
      "Evaluating at step 64\n",
      "Validation Loss: 6.0547, Accuracy: 0.3080\n",
      "Evaluating at step 80\n",
      "Validation Loss: 6.1867, Accuracy: 0.3040\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.0046, Accuracy: 0.3420\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.1881, Accuracy: 0.3620\n",
      "Training Loss: 0.7765\n",
      "================\n",
      "Starting epoch 13/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 5.0770, Accuracy: 0.3960\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.7301, Accuracy: 0.3060\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.7340, Accuracy: 0.3440\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.8777, Accuracy: 0.3280\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.3802, Accuracy: 0.3220\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.6055, Accuracy: 0.3020\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.5436, Accuracy: 0.3240\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.3907, Accuracy: 0.3140\n",
      "Training Loss: 0.7261\n",
      "================\n",
      "Starting epoch 14/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 5.4623, Accuracy: 0.3160\n",
      "Evaluating at step 16\n",
      "Validation Loss: 5.6811, Accuracy: 0.2840\n",
      "Evaluating at step 32\n",
      "Validation Loss: 5.5652, Accuracy: 0.3400\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.8298, Accuracy: 0.2800\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.5006, Accuracy: 0.3160\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.7314, Accuracy: 0.3260\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.5591, Accuracy: 0.3520\n",
      "Evaluating at step 112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.2138, Accuracy: 0.3220\n",
      "Training Loss: 0.6379\n",
      "================\n",
      "Starting epoch 15/15\n",
      "================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 5.3428, Accuracy: 0.3040\n",
      "Evaluating at step 16\n",
      "Validation Loss: 6.2661, Accuracy: 0.3000\n",
      "Evaluating at step 32\n",
      "Validation Loss: 6.2808, Accuracy: 0.3100\n",
      "Evaluating at step 48\n",
      "Validation Loss: 5.9231, Accuracy: 0.3600\n",
      "Evaluating at step 64\n",
      "Validation Loss: 5.5888, Accuracy: 0.3180\n",
      "Evaluating at step 80\n",
      "Validation Loss: 5.4860, Accuracy: 0.3660\n",
      "Evaluating at step 96\n",
      "Validation Loss: 5.4858, Accuracy: 0.3840\n",
      "Evaluating at step 112\n",
      "Validation Loss: 5.7943, Accuracy: 0.3620\n",
      "Training Loss: 0.5941\n",
      "Training complete..\n",
      "CPU times: user 8h 12min 49s, sys: 11min 28s, total: 8h 24min 18s\n",
      "Wall time: 31min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "accuracy_list, val_loss_list, train_loss_list = train(epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "HmBdVqmeE9VU",
    "outputId": "213ac4b3-f402-41e5-a7d2-96a0a832ca26"
   },
   "outputs": [],
   "source": [
    "model = best_model\n",
    "model = model.to('cpu')\n",
    "model.eval() # set model to eval phase\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "for test_step, (images, question, _,  labels) in enumerate(dl_test):\n",
    "\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    y_true.append(labels)\n",
    "    y_pred.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "H7VQL5PgiZq8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_true: (500,)\n",
      "(500,)\n",
      "(500, 332)\n",
      "(500, 332)\n"
     ]
    }
   ],
   "source": [
    "num_classes=332\n",
    "Y_true=torch.cat((torch.flatten(torch.stack(y_true[:-1])),y_true[-1]),dim=0)\n",
    "Y_pred=torch.stack(y_pred[:-1])\n",
    "Y_pred=torch.reshape(torch.stack(y_pred[:-1]),(Y_pred.shape[0]*Y_pred.shape[1],num_classes))\n",
    "Y_pred=torch.cat((Y_pred,y_pred[-1]),dim=0)\n",
    "Y_prob=Y_pred\n",
    "_, Y_pred = torch.max(Y_pred, 1)\n",
    "Y_true=Y_true.to('cpu')\n",
    "Y_pred=Y_pred.to('cpu')\n",
    "Y_prob=Y_prob.to('cpu')\n",
    "\n",
    "Y_true=Y_true.numpy()\n",
    "Y_pred=Y_pred.numpy()\n",
    "Y_prob=Y_prob.detach().numpy()\n",
    "Y_prob=tf.nn.softmax(Y_prob)\n",
    "Y_true_oh=tf.keras.utils.to_categorical(Y_true, num_classes=332)\n",
    "print('Y_true:', Y_true.shape)\n",
    "print(Y_pred.shape)\n",
    "print(Y_prob.shape)\n",
    "print(Y_true_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return metrics.roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "DVsGgRItx0Bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956222601297944"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_roc_auc_score(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "x7T_25456_gD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.37043311133311135\n",
      "Accuracy:  0.408\n"
     ]
    }
   ],
   "source": [
    "acc=np.mean(Y_true==Y_pred)\n",
    "F1=metrics.f1_score(Y_true, Y_pred, average='weighted')\n",
    "\n",
    "print('F1 Score:', F1)\n",
    "print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "d_r0qbrBAx2q"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'DenseNet_UnF.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DenseNet121-MoCo_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
