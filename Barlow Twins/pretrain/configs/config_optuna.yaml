# @package _global_

# example hyperparameter optimization of some experiment with optuna:
# python train.py -m --config-name config_optuna.yaml +experiment=exp_example_simple logger=wandb

defaults:
    # load everything from main config file
    - config.yaml

    # override sweeper to Optuna!
    - override hydra/sweeper: optuna


# choose metric which will be optimized by optuna
optimized_metric: "train/loss"


hydra:
    # here we define optuna objective
    # it optimizes for value returned from function with @hydra.main decorator
    # learn more here: https://hydra.cc/docs/next/plugins/optuna_sweeper
    sweeper:
        optuna_config:
            study_name: null
            storage: null
            n_jobs: 1
            seed: 12345

            # 'minimize' or 'maximize' the objective
            direction: minimize

            # number of experiments that will be executed
            n_trials: 20

            # choose optuna hyperparameter sampler ('tpe', 'random', 'cmaes' or 'nsgaii', 'motpe')
            # learn more here: https://optuna.readthedocs.io/en/stable/reference/samplers.html
            sampler: tpe

        # define range of hyperparameters
        search_space:
            datamodule.batch_size:
                type: categorical
                choices: [32, 64, 128]
            optimizer.lr:
                type: float
                low: 1e-3
                high: 2
            model.proj_head_dim:
                type: categorical
                choices: [256, 512, 1024, 2048]
            model.proj_head_out:
                type: categorical
                choices: [256, 512, 1024, 2048]
            model.lambda_param:
                type: float
                low: 1e-3
                high: 1e-2
            
